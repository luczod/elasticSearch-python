{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Documentation\n",
                "\n",
                "To read more about the search after parameter, checkout the docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/8.15/paginate-search-results.html#search-after).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Connect to ElasticSearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pprint import pprint\n",
                "from elasticsearch import Elasticsearch\n",
                "\n",
                "\n",
                "es = Elasticsearch('http://localhost:9200')\n",
                "client_info = es.info()\n",
                "print('Connected to Elasticsearch!')\n",
                "pprint(client_info.body)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preparing the index"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `timestamp` field is useful for sorting documents, which is essential for the `search_after` parameter. Alternatively, you can use the document ID for sorting as well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index_name = 'my_index'\n",
                "mapping = {\n",
                "    \"mappings\": {\n",
                "        \"properties\": {\n",
                "            \"timestamp\": {\"type\": \"date\"},\n",
                "            \"value\": {\"type\": \"float\"},\n",
                "            \"category\": {\"type\": \"keyword\"},\n",
                "            \"description\": {\"type\": \"text\"},\n",
                "            \"id\": {\"type\": \"keyword\"},\n",
                "        }\n",
                "    },\n",
                "}\n",
                "\n",
                "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
                "es.indices.create(index=index_name, body=mapping)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generating fake data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The base documents will be duplicated to create a total of `100,000` documents. This is done to compare the `from/size` method with the `search_after` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_documents = [\n",
                "    {\n",
                "        \"category\": \"A\",\n",
                "        \"value\": 100,\n",
                "        \"description\": \"First sample document\"\n",
                "    },\n",
                "    {\n",
                "        \"category\": \"B\",\n",
                "        \"value\": 200,\n",
                "        \"description\": \"Second sample document\"\n",
                "    },\n",
                "    {\n",
                "        \"category\": \"C\",\n",
                "        \"value\": 300,\n",
                "        \"description\": \"Third sample document\"\n",
                "    },\n",
                "    {\n",
                "        \"category\": \"D\",\n",
                "        \"value\": 400,\n",
                "        \"description\": \"Fourth sample document\"\n",
                "    },\n",
                "    {\n",
                "        \"category\": \"E\",\n",
                "        \"value\": 500,\n",
                "        \"description\": \"Fifth sample document\"\n",
                "    }\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `generate_bulk_data` function determines the number of times to duplicate the base documents to achieve a target of `100,000` documents. It also assigns a unique `_id`, modifies the `value` field randomly, and appends a `timestamp` to each duplicated document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "\n",
                "def generate_bulk_data(base_documents, target_size=100_000):\n",
                "    documents = []\n",
                "    base_count = len(base_documents)\n",
                "    duplications_needed = target_size // base_count\n",
                "\n",
                "    base_timestamp = datetime.now()\n",
                "\n",
                "    for i in range(duplications_needed):\n",
                "        for document in base_documents:\n",
                "            new_doc = document.copy()\n",
                "            new_doc['id'] = f\"doc_{len(documents)}\"\n",
                "            new_doc['timestamp'] = (\n",
                "                base_timestamp - timedelta(minutes=len(documents))).isoformat()\n",
                "            new_doc['value'] = document['value'] + random.uniform(-10, 10)\n",
                "            documents.append(new_doc)\n",
                "\n",
                "    return documents\n",
                "\n",
                "\n",
                "documents = generate_bulk_data(base_documents, target_size=100_000)\n",
                "print(f\"Generated {len(documents)} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Indexing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "operations = []\n",
                "for document in tqdm(documents, total=len(documents)):\n",
                "    operations.append({'index': {'_index': index_name}})\n",
                "    operations.append(document)\n",
                "\n",
                "response = es.bulk(operations=operations)\n",
                "pprint(response.body[\"errors\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "es.indices.refresh(index=index_name)\n",
                "\n",
                "count = es.count(index=index_name)[\"count\"]\n",
                "print(f\"Indexed {count} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## From / Size method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To use the `from/size` method, include two parameters in your query: `from`, which specifies the number of documents to skip, and `size`, which tells Elasticsearch how many documents to return."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = es.search(\n",
                "    index=index_name,\n",
                "    body={\n",
                "        \"from\": 0,\n",
                "        \"size\": 10,\n",
                "        \"sort\": [\n",
                "            {\"timestamp\": \"desc\"},\n",
                "            {\"id\": \"desc\"}\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "\n",
                "hits = response[\"hits\"][\"hits\"]\n",
                "for hit in hits:\n",
                "    print(f\"ID: {hit['_source']['id']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To retrieve the next batch of documents, adjust the `from` parameter from 0 to 10."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = es.search(\n",
                "    index=index_name,\n",
                "    body={\n",
                "        \"from\": 10,\n",
                "        \"size\": 10,\n",
                "        \"sort\": [\n",
                "            {\"timestamp\": \"desc\"},\n",
                "            {\"id\": \"desc\"}\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "\n",
                "hits = response[\"hits\"][\"hits\"]\n",
                "for hit in hits:\n",
                "    print(f\"ID: {hit['_source']['id']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Search after method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To use the `search_after` method, include the following parameters in your query:\n",
                "\n",
                "1. **size**: Specifies the number of documents to retrieve in each batch, similar to the `size` parameter in `from/size`.\n",
                "\n",
                "2. **sort**: The `search_after` method requires specifying one or more fields to sort the results, such as `timestamp` or `id`. Sorting ensures a consistent order for navigating through result pages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = es.search(\n",
                "    index=index_name,\n",
                "    body={\n",
                "        \"size\": 10,\n",
                "        \"sort\": [\n",
                "            {\"timestamp\": \"desc\"},\n",
                "            {\"id\": \"desc\"}\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "\n",
                "hits = response[\"hits\"][\"hits\"]\n",
                "for hit in hits:\n",
                "    print(f\"ID: {hit['_source']['id']}\")\n",
                "    print(f\"Sort values: {hit['sort']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To retrieve the next batch of documents using `search_after`, youâ€™ll pass the `sort` values from the last document of the previous batch to the `search_after` parameter in the subsequent query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "last_sort_values = hits[-1][\"sort\"]\n",
                "response = es.search(\n",
                "    index=index_name,\n",
                "    body={\n",
                "        \"size\": 10,\n",
                "        \"sort\": [\n",
                "            {\"timestamp\": \"desc\"},\n",
                "            {\"id\": \"desc\"}\n",
                "        ],\n",
                "        \"search_after\": last_sort_values\n",
                "    }\n",
                ")\n",
                "\n",
                "hits = response[\"hits\"][\"hits\"]\n",
                "for hit in hits:\n",
                "    print(f\"ID: {hit['_source']['id']}\")\n",
                "    print(f\"Sort values: {hit['sort']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Benchmark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this benchmark, we assess the performance of two pagination methods, `from/size` and `search_after`, by measuring and comparing their response times. We capture the response time of each method for multiple pages, plot the results, and calculate relevant statistics to provide insights."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. From / Size test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "from tqdm import tqdm\n",
                "\n",
                "\n",
                "def test_from_size_pagination(es: Elasticsearch, index_name: str, page_size=100, max_pages=50)-> list[float]:\n",
                "    timings = []\n",
                "\n",
                "    for page in tqdm(range(max_pages)):\n",
                "        start_time = time.time()\n",
                "\n",
                "        _ = es.search(\n",
                "            index=index_name,\n",
                "            body={\n",
                "                \"from\": page * page_size,\n",
                "                \"size\": page_size,\n",
                "                \"sort\": [\n",
                "                    {\"timestamp\": \"desc\"},\n",
                "                    {\"id\": \"desc\"}\n",
                "                ]\n",
                "            }\n",
                "        )\n",
                "\n",
                "        end_time = time.time()\n",
                "        final_time = (end_time - start_time) * 1000\n",
                "        timings.append((page + 1, final_time))\n",
                "\n",
                "    return timings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When attempting to retrieve more than 10,000 documents, Elasticsearch returns an error indicating that the `from / size` method cannot handle this request."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from_size_timings = test_from_size_pagination(\n",
                "    es=es,\n",
                "    index_name=index_name,\n",
                "    page_size=1000,\n",
                "    max_pages=50\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's reduce the `page_size` to avoid this problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from_size_timings = test_from_size_pagination(\n",
                "    es=es,\n",
                "    index_name=index_name,\n",
                "    page_size=200,\n",
                "    max_pages=50\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Search after test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_search_after_pagination(es: Elasticsearch, index_name: str, page_size=100, max_pages=50) -> list[float]:\n",
                "    timings = []\n",
                "    search_after = None\n",
                "\n",
                "    for page in tqdm(range(max_pages)):\n",
                "        start_time = time.time()\n",
                "\n",
                "        body = {\n",
                "            \"size\": page_size,\n",
                "            \"sort\": [\n",
                "                {\"timestamp\": \"desc\"},\n",
                "                {\"id\": \"desc\"}\n",
                "            ]\n",
                "        }\n",
                "\n",
                "        if search_after:\n",
                "            body[\"search_after\"] = search_after\n",
                "\n",
                "        response = es.search(\n",
                "            index=index_name,\n",
                "            body=body\n",
                "        )\n",
                "\n",
                "        hits = response[\"hits\"][\"hits\"]\n",
                "        if hits:\n",
                "            search_after = hits[-1][\"sort\"]\n",
                "\n",
                "        end_time = time.time()\n",
                "        final_time = (end_time - start_time) * 1000\n",
                "        timings.append((page + 1, final_time))\n",
                "\n",
                "    return timings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "search_after_timings = test_search_after_pagination(\n",
                "    es, index_name, page_size=200, max_pages=50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Plotting & statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "def plot_comparison(from_size_timings: list[float], search_after_timings: list[float]) -> None:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "\n",
                "    pages_from_size, times_from_size = zip(*from_size_timings)\n",
                "    pages_search_after, times_search_after = zip(*search_after_timings)\n",
                "\n",
                "    plt.plot(pages_from_size, times_from_size, 'b-', label='from/size')\n",
                "    plt.plot(pages_search_after, times_search_after,\n",
                "             'g-', label='search_after')\n",
                "\n",
                "    plt.xlabel('Page number')\n",
                "    plt.ylabel('Response time (milliseconds)')\n",
                "    plt.title('Pagination performance comparison')\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "plot_comparison(from_size_timings, search_after_timings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `search_after` method performs more efficiently, especially for deep pagination, due to its stable response time. In contrast, `from/size` may be suitable for shallow pagination but becomes inefficient as the page depth grows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import TypedDict\n",
                "\n",
                "class TimeStats(TypedDict):\n",
                "    avg_time: float\n",
                "    max_time: float\n",
                "    min_time: float\n",
                "\n",
                "# Define the type for the outer dictionary\n",
                "class PagingStats(TypedDict):\n",
                "    from_size: TimeStats\n",
                "    search_after: TimeStats\n",
                "\n",
                "\n",
                "def calculate_stats(from_size_timings: list[float], search_after_timings: list[float]) -> PagingStats:\n",
                "    _, times_from_size = zip(*from_size_timings)\n",
                "    _, times_search_after = zip(*search_after_timings)\n",
                "\n",
                "    stats = {\n",
                "        'from_size': {\n",
                "            'avg_time': sum(times_from_size) / len(times_from_size),\n",
                "            'max_time': max(times_from_size),\n",
                "            'min_time': min(times_from_size)\n",
                "        },\n",
                "        'search_after': {\n",
                "            'avg_time': sum(times_search_after) / len(times_search_after),\n",
                "            'max_time': max(times_search_after),\n",
                "            'min_time': min(times_search_after)\n",
                "        }\n",
                "    }\n",
                "    return stats\n",
                "\n",
                "\n",
                "stats = calculate_stats(from_size_timings, search_after_timings)\n",
                "\n",
                "print(\"\\nPerformance statistics:\")\n",
                "print(\"\\n- From/Size pagination:\")\n",
                "print(f\"Average time: {stats['from_size']['avg_time']:.3f} milliseconds\")\n",
                "print(f\"Maximum time: {stats['from_size']['max_time']:.3f} milliseconds\")\n",
                "print(f\"Minimum time: {stats['from_size']['min_time']:.3f} milliseconds\")\n",
                "\n",
                "print(\"\\n- Search after pagination:\")\n",
                "print(f\"Average time: {stats['search_after']['avg_time']:.3f} milliseconds\")\n",
                "print(f\"Maximum time: {stats['search_after']['max_time']:.3f} milliseconds\")\n",
                "print(f\"Minimum time: {stats['search_after']['min_time']:.3f} milliseconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These statistics validate that `search_after` is the preferable pagination method for consistent and scalable performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "_, times_from_size = zip(*from_size_timings)\n",
                "_, times_search_after = zip(*search_after_timings)\n",
                "\n",
                "plt.hist(times_from_size, alpha=0.5, label='from/size', bins=20)\n",
                "plt.hist(times_search_after, alpha=0.5, label='search_after', bins=20)\n",
                "plt.xlabel('Response time (milliseconds)')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Distribution of response times')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Based on this histogram visualization, the `search_after` approach (shown in orange) demonstrates consistently faster response times clustered around 2-5 milliseconds, while the `from/size` method (in blue) shows a wider distribution of response times spreading up to 16 milliseconds, suggesting that `search_after` provides more predictable and generally better performance for pagination."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_degradation(timings:float)-> float:\n",
                "    first_page_time = timings[0][1]\n",
                "    last_page_time = timings[-1][1]\n",
                "    degradation_factor = last_page_time / first_page_time\n",
                "    return degradation_factor\n",
                "\n",
                "\n",
                "from_size_degradation = calculate_degradation(from_size_timings)\n",
                "search_after_degradation = calculate_degradation(search_after_timings)\n",
                "\n",
                "print(\"\\nPerformance degradation (Last page time / First page time):\")\n",
                "print(f\"- From/Size degradation factor   : {from_size_degradation:.2f}x\")\n",
                "print(f\"- Search after degradation factor: {search_after_degradation:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `search_after` method is far superior in maintaining a stable response time, even for large page numbers. In contrast, `from/size` exhibits significant performance degradation, making it less suitable for deep pagination.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For larger indexes, it's recommended to use the `search_after` method. For smaller indexes, both methods work well."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
